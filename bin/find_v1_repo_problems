#!/usr/bin/env python3

# Examine one or more AUs for URLs that the V1 repository doesn't represent
# correctly.  It does this by comparing the URL generated by the
# repository's iterator with the X-Lockss-node-url property, which recorded
# the original URL supplied to the repository.  These differences are
# detected:
# - dropped final slash
# - encoding differences (repo performs minimal encoding necessary for
#   filesystem)
# - inconsistencies between versions (e.g, if both slash and non-slash
#   variants were collected for the same URL

import sys
import argparse
import logging
import requests
import os
import json
import urllib.parse

# fetch an item list from ListObjects
def get_list(argdict):

    # prevent ListObjects from returning html error pages
    argdict['errorResp'] = 'text'

    url = url_pat % urllib.parse.urlencode(argdict)
    resp = requests.get(url, stream=True,
                        auth=requests.auth.HTTPBasicAuth(user, passwd))
    # throw if non-200 status
    resp.raise_for_status()
    if resp.encoding is None:
        resp.encoding = 'utf-8'

    # return non-comment lines
    return [line for line in resp.iter_lines(decode_unicode=True)
            if line and not str.startswith(line, '#')]
    

# return list of (auid, auname) tuples 
def find_auids():
    lines = get_list({'type': 'aus',
                      'fields': 'Auid,AuName'})
    return [ line.split('\t') for line in lines ]


def printifnz(file, cnt, msg):
    if args.verbose or cnt != 0:
        out.write(msg + '\n')

def cpfile():
    return '%s.auids' % args.hostport

def outfile():
    return '%s.out' % args.hostport

def errfile():
    return '%s.err' % args.hostport

def write_auids(auids):
    with open(cpfile(),"w") as f:
        json.dump(auids, f)

def read_auids():
    with open(cpfile(), "r") as f:
        return json.load(f)

def del_auids_file():
    os.remove(cpfile())

parser = argparse.ArgumentParser(description='Find CUs with V1 repository specific (incorrect) names.')
parser.add_argument('hostport', help='HOST:PORT')
parser.add_argument('-l', '--loglevel', default='WARNING', help='set log level')
parser.add_argument('-u', '--userpass', metavar='USER:PASS', required=True)
parser.add_argument('--auid', nargs='*',
                    help='one or more (non-urlencoded) AUIDs. If none supplied, all active AUs will be searched')
parser.add_argument('--resume', action='store_true',
                    help='resume processing AUIDs in <host:port>.auids file')
parser.add_argument('-mv', '--maxversions', type=int, default=0,
                    help='Optionally limit number of versions of each URL examined for inter-version discrepencies. Default is no limit.')
parser.add_argument('-/', '--noslash', action='store_true', help='Count but do not display URLs missing only final slash')
parser.add_argument('-v', '--verbose', action='store_true', help='Log totals for AUs that have no discrepancies')

args = parser.parse_args()
#print(args)

log_level = getattr(logging, args.loglevel.upper(), None)
if not isinstance(log_level, int):
    raise ValueError('Invalid log level: %s' % args.loglevel)
logging.basicConfig(level=log_level)

# The above works to set the root log level, but some libraries log
# debugging messages at info level, so configuring everything to info is a
# loser.  Setting individual log levels is broken - setting it higher works
# to suppress lower level message, but setting it lower does not result in
# info messages being output.  Result: everything that should be logged at
# info level here is instead logged as a warning.

# log = logging.getLogger(__name__)
# log.setLevel(log_level)


url_pat = 'http://%s/ListObjects?%%s' % args.hostport

user, ignore, passwd = str.partition(args.userpass, ':')

first = not os.path.exists(outfile()) or os.stat(outfile()).st_size == 0

append_or_write = 'a'

if args.auid:
    # if auids in args, turn into list of (auid, None) tuples
    auids = [ (x, None) for x in args.auid ]
    auids.reverse()             # so can pop efficiently
    os.remove(outfile())
elif args.resume:
    if os.path.exists(cpfile()):
        auids = read_auids()
        if not auids:
            logging.warning('AUIDs file empty, done!')
            del_auids_file()
            sys.exit(0)
    else:
        logging.warning('No AUIDs file, can\'t resume')
        sys.exit(1)
else:
    # else get AUID list from daemon
    if os.path.exists(cpfile()):
        logging.error('Checkpoint file exists.  If you really want to start afresh you must first delete %s' % cpfile())
        sys.exit(1)
    try:
        auids = find_auids()
        auids.reverse()         # so can pop efficiently
        append_or_write = 'w'
        first = True
    except Exception as e:
        logging.error('Couldn\'t retrieve AUID list: %s' % e)
        sys.exit(1)
        
if not auids:
    print('# No AUIDs found')
    sys.exit(1)

write_auids(auids)
logging.debug('AUIDs: %s' % auids)

with open(outfile(), append_or_write) as out, open(errfile(), append_or_write) as err:
    while auids:
        (auid, auname) = auids.pop(0)
        first_err = True;
        try:
            lines = get_list({'type': 'urls',
                              'auid': auid,
                              'fields': 'PropsUrl,Version',
                              'maxversions': args.maxversions})
            msgs = []
            numslash=0
            numother=0
            numalts=0
            alts = set()            # accumulates variants of single url

            prevurl = None
            for line in lines:
                logging.debug('line: %s' % line)
                try:
                    url, realUrl, ver = line.split()
                except ValueError:
                    if first_err:
                        err.write('Errors for %s\n%s\n' % (auname, auid) )
                        first_err = False;
                    err.write('line: %s\n' % line)
                    err.flush()
                    continue

                if url != prevurl:
                    # if prev url had more than one variant, record it
                    if len(alts) > 1:
                        msgs.append('VARIANTS:\t%s\t%s' % (prevurl, alts))
                        numalts += 1

                    prevurl = url
                    alts = set()
                alts.add(realUrl)

                if url != realUrl:
                    if realUrl.endswith('/') and url == realUrl[0:-1]:
                        # missing trailing slash
                        numslash += 1
                        if not args.noslash:
                            msgs.append('SLASH:\t%s\t%s' % (url, realUrl))
                    else:
                        # other difference - encoding?
                        numother += 1
                        msgs.append('OTHER:\t%s\t%s' % (url, realUrl))

            # if final url had more than one variant, record it
            if len(alts) > 1:
                msgs.append('VARIANTS:\t%s\t%s' % (url, alts))
                numalts += 1

            if args.verbose or msgs or numalts > 1 or not (numslash == numother == 0):
                if first:
                    first=False
                else:
                    out.write('\n')

                out.write('AUID: %s\n' % auid)
                if auname:
                    out.write('AUNAME: %s\n' % auname)
                for line in msgs:
                    out.write(line + '\n')

            printifnz(out, numslash, 'TOTAL SLASH: %d' % numslash)
            printifnz(out, numother, 'TOTAL OTHER: %d' % numother)
            printifnz(out, numalts, 'TOTAL VARIANTS: %d' % numalts)

        except requests.exceptions.HTTPError:
            out.write('ERROR: AUID not found: %s\n' % auid)

        out.flush()
        if auids:
            # checkpoint auid, every 10 times when it's huge
            if len(auids) < 10000 or len(auids) % 10 == 0:
                write_auids(auids)
        else:
            del_auids_file()
            logging.warning('Done with %s' % args.hostport)
