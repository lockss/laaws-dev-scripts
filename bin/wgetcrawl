#!/usr/bin/env bash

trap cleanup SIGINT SIGTERM ERR EXIT

script_dir=$(cd "$(dirname "${BASH_SOURCE[0]}")" &>/dev/null && pwd -P)
script_name=$(basename "${BASH_SOURCE[0]}")

# ---- defaults
depth=99
wait=""
host=localhost

usage() {
  cat <<EOF
Usage: ${script_name} [-h] --user <user:pass> --handle handle --host hostname --start url(s) --depth depth --wait msecs

Run a single wget crawl using the LOCKSS Crawler Service.

Available options:

-h, --help     Print this help and exit
    --user     The user:password needed to connect to the crawler service.
    --handle   The handle to use for generating a AU id.
    --host     The hostname of the crawler service (default localhost)
    --start    The url(s) from which the crawl should start: url1(;url2)
    --depth    The crawl depth (default 99)
    --wait     The number of milliseconds to wait between fetches.
               (Default from system config, which defaults to 3 seconds.)
EOF
  exit
}

cleanup() {
  trap - SIGINT SIGTERM ERR EXIT
  # script cleanup here
}

msg() {
  echo >&2 -e "${1-}"
}

die() {
  local msg=$1
  local code=${2-1} # default exit status 1
  msg "$msg"
  exit "$code"
}

while [[ $# -gt 0 ]]; do
  case "$1" in
  -h | --help) usage ;;
  "--user")
    user="${2}"
    shift; shift; continue;;
  "--handle")
    handle="${2}"
    shift; shift; continue;;
  "--host" )
    host="${2}"
    shift; shift; continue;;
  "--start" )
    starturl="${2}"
    shift; shift; continue;;
  "--depth" )
    depth="${2}"
    shift; shift; continue;;
  "--wait" )
    wait="${2}"
    shift; shift; continue;;
  -*) usage
  esac
done

if [ -z "${user}" -o -z "${handle}" -o -z "${starturl}" ]; then
  echo "user:pass, handle, and start URL must be provided"
  usage
fi

#echo "user: ${user}"
echo "host: ${host}"
echo "starturl: ${starturl}"

curlcmd="curl -s -S -u ${user}"

err_exit() {
  echo "$1" >&2
  exit 1
}

url_encode() {
  python3 -c "import urllib.parse; print (urllib.parse.quote('''$1'''))"
}

is_svc_ready() {
  local port="$1"
  local api_status=`( ${curlcmd} -X GET --header 'Accept: application/json' "http://${host}:${port}/status" ) || err_exit "Couldn't get service status"`
  echo "${api_status}" | jq -r .startupStatus | grep -q AUS_STARTED
}

wait_svc_ready() {
  local port="$1"
  local svc_name="$2"
  local did=""
  while ! is_svc_ready "${port}"; do
    if [ -z "${did}" ]; then
      echo -n "Waiting for ${svc_name} to be ready ..."
      did=1
    fi
    sleep 5
  done
  if [ -n "${did}" ]; then
    echo " done."
  fi
}

is_au_configured() {
  local auid="$1"
  local encoded_auid="`url_encode ${auid}`"
  local cur_config=`( ${curlcmd} -X GET --header 'Accept: application/json' "http://${host}:24620/aus/${encoded_auid}" ) || err_exit "Couldn't check current AU config"`
  echo "${cur_config}" | grep -q auId
}

wait_svc_ready 24620 "Config Service"
wait_svc_ready 24660 "Crawler Service"

# get AUID
auid=`${curlcmd} -X POST --header "Content-Type: application/x-www-form-urlencoded" --header "Accept: application/json" --data-urlencode "handle=${handle}" "http://${host}:24620/auids" | jq -r .auid`
echo "auid: ${auid}"

if ! is_au_configured "${auid}"; then
  # configure AU
  config="{\"auConfig\": {\"handle\": \"${handle}\", \"features\": \"crawledAu\"}, \"auId\": \"${auid}\"}"
  ${curlcmd} -X PUT --header "Content-Type: application/json" --header "Accept: application/json" -d "${config}" "http://${host}:24620/aus/AUID"
  sleep 5
else
  echo "AU already configured"
fi

# start crawl
if ${curlcmd} -X POST --header "Content-Type: application/json" --header "Accept: application/json" -d "{\"auId\": \"${auid}\", \"crawlDepth\":${depth}, \"crawlKind\": \"newContent\", \"crawlList\":[\"${starturl}\"], \"crawlerId\": \"wget\", \"extraCrawlerData\": {\"span-hosts\":false, \"page-requisites\": true, \"no-parent\": true, \"wait\":\"${wait}\"}, \"forceCrawl\": false, \"priority\":0, \"refetchDepth\":0}" "http://${host}:24660/jobs"; then
  echo
  echo "Crawl requested"
else
  echo
  echo "Crawl request failed"
fi
